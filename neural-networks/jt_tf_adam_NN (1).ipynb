{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jt-tf-adam-NN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Nnb-Gqd81h1a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import Packages"
      ]
    },
    {
      "metadata": {
        "id": "NxnrDQg41SUE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "a150ee84-d20b-4f81-a212-6daae73677f5"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "#old_v = tf.logging.get_verbosity()\n",
        "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "!pip install tensorboardcolab\n",
        "from tensorboardcolab import *\n",
        "tbc = TensorBoardColab()\n",
        "\n",
        "\n",
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "http://b3d04238.ngrok.io\n",
            "WARNING:tensorflow:From <ipython-input-1-c345f273dc79>:10: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "870PnZbE1lY_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Loading Functions"
      ]
    },
    {
      "metadata": {
        "id": "Snp6Uqnb1V3T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def TRAIN_SIZE(num):\n",
        "    print ('Total Training Images in Dataset = ' + str(mnist.train.images.shape))\n",
        "    print ('--------------------------------------------------')\n",
        "    x_train = mnist.train.images[:num,:]\n",
        "    print ('x_train Examples Loaded = ' + str(x_train.shape))\n",
        "    y_train = mnist.train.labels[:num,:]\n",
        "    print ('y_train Examples Loaded = ' + str(y_train.shape))\n",
        "    print('')\n",
        "    return x_train, y_train\n",
        "\n",
        "def TEST_SIZE(num):\n",
        "    print ('Total Test Examples in Dataset = ' + str(mnist.test.images.shape))\n",
        "    print ('--------------------------------------------------')\n",
        "    x_test = mnist.test.images[:num,:]\n",
        "    print ('x_test Examples Loaded = ' + str(x_test.shape))\n",
        "    y_test = mnist.test.labels[:num,:]\n",
        "    print ('y_test Examples Loaded = ' + str(y_test.shape))\n",
        "    return x_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OLPTGVXQVn6O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Parameters"
      ]
    },
    {
      "metadata": {
        "id": "iNURoqHd1YiF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "4baf186d-8e53-410a-acbb-312b464a6d58"
      },
      "cell_type": "code",
      "source": [
        "x_train_m2, y_train_m2 = TRAIN_SIZE(30000)\n",
        "x_test_m2, y_test_m2 = TEST_SIZE(10000)\n",
        "\n",
        "# Define model parameters\n",
        "learning_rate_m1 = 0.001\n",
        "training_epochs_m1 = 100\n",
        "\n",
        "number_of_inputs_m1 = 784\n",
        "number_of_outputs_m1 = 10\n",
        "\n",
        "# define how many nuerons in hidden layer\n",
        "layer_1_nodes_m2 = 100"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Training Images in Dataset = (55000, 784)\n",
            "--------------------------------------------------\n",
            "x_train Examples Loaded = (30000, 784)\n",
            "y_train Examples Loaded = (30000, 10)\n",
            "\n",
            "Total Test Examples in Dataset = (10000, 784)\n",
            "--------------------------------------------------\n",
            "x_test Examples Loaded = (10000, 784)\n",
            "y_test Examples Loaded = (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rQglhAVEVtj3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Network"
      ]
    },
    {
      "metadata": {
        "id": "cLDNopp31Zw1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Input Layer\n",
        "with tf.variable_scope('Input_Layer', reuse=tf.AUTO_REUSE):\n",
        "    X = tf.placeholder(tf.float32, shape=(None, number_of_inputs_m1))\n",
        "\n",
        "# Hidden Layer\n",
        "with tf.variable_scope('Hidden_Layer',reuse=tf.AUTO_REUSE):\n",
        "    weights = tf.get_variable(\"weights1\", shape=[number_of_inputs_m1, layer_1_nodes_m2], initializer=tf.contrib.layers.xavier_initializer())\n",
        "    biases = tf.get_variable(name=\"biases1\", shape=[layer_1_nodes_m2], initializer=tf.zeros_initializer())\n",
        "    layer_1_output = tf.nn.relu(tf.matmul(X, weights) + biases)\n",
        "    \n",
        "# Output Layer\n",
        "with tf.variable_scope('Output_Layer',reuse=tf.AUTO_REUSE):\n",
        "    weights = tf.get_variable(\"weights2\", shape=[layer_1_nodes_m2, number_of_outputs_m1], initializer=tf.contrib.layers.xavier_initializer())\n",
        "    biases = tf.get_variable(name=\"biases2\", shape=[number_of_outputs_m1], initializer=tf.zeros_initializer())\n",
        "    prediction = tf.nn.softmax(tf.matmul(layer_1_output, weights) + biases)\n",
        "\n",
        "# define Y and cost function\n",
        "with tf.variable_scope('Cost_Function',reuse=tf.AUTO_REUSE):\n",
        "    Y = tf.placeholder(tf.float32, shape=(None, 10))\n",
        "    \n",
        "    cost = tf.reduce_mean(tf.squared_difference(prediction, Y))\n",
        "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=X, logits=Y))\n",
        "    \n",
        "# define optimizer\n",
        "with tf.variable_scope('Optimizer',reuse=tf.AUTO_REUSE):\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate_m1).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UInGKAn2Vw2A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Output"
      ]
    },
    {
      "metadata": {
        "id": "0yrdODzi1emY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "fe7273d4-6f07-4f59-dc69-5299502a85c7"
      },
      "cell_type": "code",
      "source": [
        "# Create a summary operation to log the progress of the network\n",
        "with tf.variable_scope('logging_m2',reuse=tf.AUTO_REUSE):\n",
        "    tf.summary.scalar('current_cost', cost)\n",
        "    summary = tf.summary.merge_all()\n",
        "    \n",
        "    \n",
        "# Initialize a session so that we can run TensorFlow operations\n",
        "with tf.Session() as session:\n",
        "\n",
        "    # Run the global variable initializer to initialize all variables and layers of the neural network\n",
        "    session.run(tf.global_variables_initializer())\n",
        "\n",
        "    # Create log file writers to record training progress.\n",
        "    # We'll store training and testing log data separately.\n",
        "    training_writer = tf.summary.FileWriter('./logs/training/m1', session.graph)\n",
        "    testing_writer = tf.summary.FileWriter('./logs/testing/m1', session.graph)\n",
        "    #training_writer = summary_writer('.logs/training', session.graph)\n",
        "    #testing_writer = summary_writer('./logs/testing', session.graph)\n",
        "\n",
        "    # Run the optimizer over and over to train the network.\n",
        "    # One epoch is one full run through the training data set.\n",
        "    for epoch in range(training_epochs_m1):\n",
        "\n",
        "        # Feed in the training data to our placeholders 'X' and 'Y' and do one step of neural network training\n",
        "        session.run(optimizer, feed_dict={X: x_train_m2, Y: y_train_m2})\n",
        "\n",
        "        # Every 5 training steps, log our progress\n",
        "        if epoch % 5 == 0:\n",
        "            # Get the current accuracy scores by running the \"cost\" operation on the training and test data sets\n",
        "            training_cost, training_summary = session.run([cost, summary], feed_dict={X: x_train_m2, Y: y_train_m2})\n",
        "            testing_cost, testing_summary = session.run([cost, summary], feed_dict={X: x_test_m2, Y: y_test_m2})\n",
        "            \n",
        "\n",
        "            # Write the current training status to the log files (Which we can view with TensorBoard)\n",
        "            training_writer.add_summary(training_summary, epoch)\n",
        "            testing_writer.add_summary(testing_summary, epoch)\n",
        "\n",
        "            # Print the current training status to the screen\n",
        "            print(\"Epoch: {} - Training Cost: {}  Testing Cost: {}\".format(epoch, training_cost, testing_cost))\n",
        "\n",
        "    # Training is now complete!\n",
        "    \n",
        "    # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n",
        "    final_training_cost = session.run(cost, feed_dict={X: x_train_m2, Y: y_train_m2})\n",
        "    final_testing_cost = session.run(cost, feed_dict={X: x_test_m2, Y: y_test_m2})\n",
        "    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(prediction, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    final_test_acc = session.run(accuracy, feed_dict={X: x_test_m2, Y: y_test_m2})\n",
        "\n",
        "    print(\"Final Training cost: {}\".format(final_training_cost))\n",
        "    print(\"Final Testing cost: {}\".format(final_testing_cost))\n",
        "    print(\"Final Testing Accuracy: {}\".format(final_test_acc))\n",
        "\n",
        "    # Now that the neural network is trained, let's use it to make predictions for our test data.\n",
        "    # Pass in the X testing data and run the \"prediciton\" operation\n",
        "    #Y_predicted_ = session.run(prediction, feed_dict={X: x_test})\n",
        "\n",
        "session.close()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 - Training Cost: 0.08819606900215149  Testing Cost: 0.08825217932462692\n",
            "Epoch: 5 - Training Cost: 0.0729975551366806  Testing Cost: 0.07298777252435684\n",
            "Epoch: 10 - Training Cost: 0.056383293122053146  Testing Cost: 0.05591967701911926\n",
            "Epoch: 15 - Training Cost: 0.043386563658714294  Testing Cost: 0.04241699352860451\n",
            "Epoch: 20 - Training Cost: 0.033768221735954285  Testing Cost: 0.03254327550530434\n",
            "Epoch: 25 - Training Cost: 0.027304062619805336  Testing Cost: 0.02609267085790634\n",
            "Epoch: 30 - Training Cost: 0.02304927445948124  Testing Cost: 0.021819980815052986\n",
            "Epoch: 35 - Training Cost: 0.020197071135044098  Testing Cost: 0.01906726323068142\n",
            "Epoch: 40 - Training Cost: 0.01825116202235222  Testing Cost: 0.017267558723688126\n",
            "Epoch: 45 - Training Cost: 0.016831545159220695  Testing Cost: 0.015957122668623924\n",
            "Epoch: 50 - Training Cost: 0.015726836398243904  Testing Cost: 0.01494979951530695\n",
            "Epoch: 55 - Training Cost: 0.014861992560327053  Testing Cost: 0.01415685098618269\n",
            "Epoch: 60 - Training Cost: 0.014163427986204624  Testing Cost: 0.01354875136166811\n",
            "Epoch: 65 - Training Cost: 0.013578830286860466  Testing Cost: 0.013081721030175686\n",
            "Epoch: 70 - Training Cost: 0.01307595893740654  Testing Cost: 0.012660535052418709\n",
            "Epoch: 75 - Training Cost: 0.012637253850698471  Testing Cost: 0.012303872033953667\n",
            "Epoch: 80 - Training Cost: 0.012246182188391685  Testing Cost: 0.011994972825050354\n",
            "Epoch: 85 - Training Cost: 0.011891965754330158  Testing Cost: 0.011712346225976944\n",
            "Epoch: 90 - Training Cost: 0.0115659199655056  Testing Cost: 0.011453893966972828\n",
            "Epoch: 95 - Training Cost: 0.011262188665568829  Testing Cost: 0.011214887723326683\n",
            "Final Training cost: 0.011032727546989918\n",
            "Final Testing cost: 0.011037137359380722\n",
            "Final Testing Accuracy: 0.9319000244140625\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}